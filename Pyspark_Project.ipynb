{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNat2p8Eo5jh+ArDccQQpH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahrukhvarzgani/Pyspark_Project/blob/main/Pyspark_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Spark environement**"
      ],
      "metadata": {
        "id": "lnhIcWWAfvGJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYTfAFl3DYru",
        "outputId": "85358ca4-cd14-48c9-b318-1c4864cafcb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.5.1\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark==3.5.1) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=84b88062461d35feb84c7748737ffa58e53468c75a0ff37201cca0aea70a8d20\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/13/41/f7f135ee114175605fb4f0a89e7389f3742aa6c1e1a5bcb657\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.5\n",
            "    Uninstalling pyspark-3.5.5:\n",
            "      Successfully uninstalled pyspark-3.5.5\n",
            "Successfully installed pyspark-3.5.1\n",
            "Collecting findspark==2.0.1\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"PYSPARK_HADOOP_VERSION\"] = \"3\"\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install pyspark==3.5.1\n",
        "!pip install findspark==2.0.1\n",
        "!pip install pandas==2.2.2\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting the following environment variables\n",
        "\n",
        "#Create dummy_hdfs folder\n",
        "os.makedirs(\"dummy_hdfs\",exist_ok=True)\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#Check if JAVA_HOME is set to Java 1.17\n",
        "print(os.environ[\"JAVA_HOME\"])\n",
        "\n",
        "#Check if HADOOP_HOME is set, needed for windows only\n",
        "#print(os.environ[\"HADOOP_HOME\"])\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "#Check if JAVA_HOME & HADOOP_HOME (windows only) are in the PATH\n",
        "print(os.environ[\"PATH\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-kTVfNuDc-t",
        "outputId": "b23b10d9-8be8-4a6d-8b4a-fdbdb95071e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-8-openjdk-amd64\n",
            "/usr/lib/jvm/java-8-openjdk-amd64/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "spark = SparkSession\\\n",
        "            .builder\\\n",
        "            .appName(\"SparkWriterJob\")\\\n",
        "            .config(\"spark.sql.shuffle.partitions\", 2)\\\n",
        "            .config(\"spark.default.parallelism\", 2)\\\n",
        "            .master(\"local[2]\")\\\n",
        "            .getOrCreate()\n",
        "print(spark.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcM_vjmNDmvh",
        "outputId": "661c7f53-81c5-4ade-e567-f8bf253bc272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mounting the drive**"
      ],
      "metadata": {
        "id": "1VCmD_atff66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ESrYcZcDodL",
        "outputId": "54fc8e2f-8465-4912-f789-369e7a477bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Uploading the file**"
      ],
      "metadata": {
        "id": "ga3yDqi1fjUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the file\n",
        "raw_student_data= spark\\\n",
        "                .read\\\n",
        "                .option(\"header\", \"true\")\\\n",
        "                .option(\"inferSchema\", \"true\")\\\n",
        "                .csv(\"/content/drive/MyDrive/Pyspark/datasets/student_scores.csv\")\n",
        "\n",
        "raw_student_data.printSchema()\n",
        "raw_student_data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvO4FjLsExxP",
        "outputId": "2e85eee6-b2d1-4d75-d47b-8daaefa355be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Student: string (nullable = true)\n",
            " |-- Subject: string (nullable = true)\n",
            " |-- ClassScore: double (nullable = true)\n",
            " |-- TestScore: double (nullable = true)\n",
            "\n",
            "+-------+---------+----------+---------+\n",
            "|Student|  Subject|ClassScore|TestScore|\n",
            "+-------+---------+----------+---------+\n",
            "|   Katy|     Math|      0.95|     2.37|\n",
            "|   Katy|Chemistry|       0.5|     1.18|\n",
            "|   Katy|  Physics|      0.48|     1.37|\n",
            "|   Katy|  Biology|      0.75|     2.79|\n",
            "|   Mike|     Math|      0.45|     1.79|\n",
            "+-------+---------+----------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the HDFS file**"
      ],
      "metadata": {
        "id": "LnycNv_dfmh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating HDFS file\n",
        "raw_student_data.write\\\n",
        "                  .option(\"compression\",\"gzip\")\\\n",
        "                  .partitionBy(\"Subject\")\\\n",
        "                  .parquet(\"dummy_hdfs/student_scores.parquet\",\n",
        "                    mode=\"Overwrite\");"
      ],
      "metadata": {
        "id": "kOFKCbQUFRgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bucketing the Student data by subject**"
      ],
      "metadata": {
        "id": "25qvCW_afCsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_student_data.write\\\n",
        "                  .format(\"parquet\")\\\n",
        "                .bucketBy(4, \"Subject\")\\\n",
        "                .saveAsTable(\"Student_Table\")\n"
      ],
      "metadata": {
        "id": "xvmTLl5kHbKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Caculating the total score"
      ],
      "metadata": {
        "id": "0Tq1PgbIVTaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "# adding the total score which comprise of ClassScore and TestScore\n",
        "total_score = raw_student_data.withColumn(\"TotalScore\", col(\"ClassScore\") + col(\"TestScore\"))\n",
        "# Total score of physics\n",
        "Physics_score = raw_student_data.where(col(\"Subject\") == \"Physics\")\n",
        "Physics_score.show()\n",
        "\n",
        "# Show the execution plan\n",
        "print(\"\\n---------Explain----------\")\n",
        "Physics_score.explain()\n",
        "print(\"--------End Explain--------\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr5qq2zOQPG1",
        "outputId": "a600d5d7-bdc0-4f3f-cb45-b85feb3271f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+----------+---------+------------------+\n",
            "|Student|Subject|ClassScore|TestScore|        TotalScore|\n",
            "+-------+-------+----------+---------+------------------+\n",
            "|   Katy|Physics|      0.48|     1.37|              1.85|\n",
            "|   Mike|Physics|      0.34|     2.72|              3.06|\n",
            "|    Bob|Physics|      0.93|     2.89|3.8200000000000003|\n",
            "|   Lisa|Physics|      0.42|     2.34|              2.76|\n",
            "|   John|Physics|      0.82|      2.8|3.6199999999999997|\n",
            "+-------+-------+----------+---------+------------------+\n",
            "\n",
            "\n",
            "---------Explain----------\n",
            "== Physical Plan ==\n",
            "*(1) Project [Student#111, Subject#112, ClassScore#113, TestScore#114, (ClassScore#113 + TestScore#114) AS TotalScore#730]\n",
            "+- *(1) Filter (isnotnull(Subject#112) AND (Subject#112 = Physics))\n",
            "   +- FileScan csv [Student#111,Subject#112,ClassScore#113,TestScore#114] Batched: false, DataFilters: [isnotnull(Subject#112), (Subject#112 = Physics)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/drive/MyDrive/Pyspark/datasets/student_scores.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Subject), EqualTo(Subject,Physics)], ReadSchema: struct<Student:string,Subject:string,ClassScore:double,TestScore:double>\n",
            "\n",
            "\n",
            "--------End Explain--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding average of total score across all the subjects of each student**"
      ],
      "metadata": {
        "id": "Ni-BQSPHWxs6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cache the total score data frame\n",
        "total_score.persist()\n",
        "average_score = total_score\\\n",
        "                          .groupBy(\"Student\")\\\n",
        "                          .agg(avg(\"TotalScore\").alias(\"AverageScore\"))\n",
        "\n",
        "\n",
        "print(\"\\n---------Explain----------\")\n",
        "average_score.explain()\n",
        "print(\"--------End Explain--------\\n\")\n",
        "average_score.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuUu5wfcWvRt",
        "outputId": "df839f21-85d6-4ebd-d2ee-ac7e09543197"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------Explain----------\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[Student#111], functions=[avg(TotalScore#761)])\n",
            "   +- Exchange hashpartitioning(Student#111, 2), ENSURE_REQUIREMENTS, [plan_id=907]\n",
            "      +- HashAggregate(keys=[Student#111], functions=[partial_avg(TotalScore#761)])\n",
            "         +- InMemoryTableScan [Student#111, TotalScore#761]\n",
            "               +- InMemoryRelation [Student#111, Subject#112, ClassScore#113, TestScore#114, TotalScore#761], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "                     +- *(1) Project [Student#111, Subject#112, ClassScore#113, TestScore#114, (ClassScore#113 + TestScore#114) AS TotalScore#761]\n",
            "                        +- FileScan csv [Student#111,Subject#112,ClassScore#113,TestScore#114] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/drive/MyDrive/Pyspark/datasets/student_scores.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Student:string,Subject:string,ClassScore:double,TestScore:double>\n",
            "\n",
            "\n",
            "--------End Explain--------\n",
            "\n",
            "+-------+------------------+\n",
            "|Student|      AverageScore|\n",
            "+-------+------------------+\n",
            "|   Mike|             2.455|\n",
            "|   Lisa|2.3899999999999997|\n",
            "|   John|            2.8525|\n",
            "|   Katy|            2.5975|\n",
            "|    Bob|             3.015|\n",
            "+-------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find top Student by Subject"
      ],
      "metadata": {
        "id": "jqtzCZdBa9Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Top score table\n",
        "top_score = total_score\\\n",
        "                .groupBy(\"Subject\")\\\n",
        "                .max(\"TotalScore\")\\\n",
        "                .withColumnRenamed(\"max(TotalScore)\", \"TotalScore\")\n",
        "\n",
        "top_score.show()\n",
        "\n",
        "# Finding the student with top score\n",
        "top_score= total_score.alias(\"a\")\\\n",
        "                .join(top_score_df.alias(\"b\"),\n",
        "                      (col(\"a.Subject\") == col(\"b.Subject\")) &\n",
        "                      (col(\"a.TotalScore\") == col(\"b.TotalScore\")))\\\n",
        "                .select(col(\"a.Student\"),\n",
        "                        col(\"a.Subject\"),\n",
        "                        col(\"a.TotalScore\"))\n",
        "\n",
        "top_score.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7He4P5Xa8Le",
        "outputId": "8eb7abf4-5d6e-40e9-f9fb-3f8c5f0797b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------------+\n",
            "|  Subject|        TotalScore|\n",
            "+---------+------------------+\n",
            "|  Biology|              3.54|\n",
            "|     Math|3.3200000000000003|\n",
            "|Chemistry|3.1999999999999997|\n",
            "|  Physics|3.8200000000000003|\n",
            "+---------+------------------+\n",
            "\n",
            "+-------+---------+------------------+\n",
            "|Student|  Subject|        TotalScore|\n",
            "+-------+---------+------------------+\n",
            "|   Katy|     Math|3.3200000000000003|\n",
            "|   Katy|  Biology|              3.54|\n",
            "|    Bob|  Physics|3.8200000000000003|\n",
            "|   John|Chemistry|3.1999999999999997|\n",
            "+-------+---------+------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}